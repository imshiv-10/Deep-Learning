{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNscM/1aXYCRHbtAYH2PDSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imshiv-10/MNIST/blob/main/LeNet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjTrwZujo1y9",
        "outputId": "cc9530c4-b8de-490b-8038-4d7e42b31734"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QCVMWbx3oZay"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class C1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(C1, self).__init__()\n",
        "\n",
        "        self.c1 = nn.Sequential(OrderedDict([\n",
        "            # filter_size and channel_size\n",
        "            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5) )),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('s1', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, img):\n",
        "        output = self.c1(img)\n",
        "        return output\n",
        "\n",
        "\n",
        "class C2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(C2, self).__init__()\n",
        "\n",
        "        self.c2 = nn.Sequential(OrderedDict([\n",
        "            ('c2', nn.Conv2d(6, 16, kernel_size=(5, 5))),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, img):\n",
        "        output = self.c2(img)\n",
        "        return output\n",
        "\n",
        "\n",
        "class C3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(C3, self).__init__()\n",
        "\n",
        "        self.c3 = nn.Sequential(OrderedDict([\n",
        "            ('c3', nn.Conv2d(16, 120, kernel_size=(5, 5))),\n",
        "            ('relu3', nn.ReLU())\n",
        "        ]))\n",
        "\n",
        "    def forward(self, img):\n",
        "        output = self.c3(img)\n",
        "        return output\n",
        "\n",
        "\n",
        "class F4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F4, self).__init__()\n",
        "\n",
        "        self.f4 = nn.Sequential(OrderedDict([\n",
        "            ('f4', nn.Linear(120, 84)),\n",
        "            ('relu4', nn.ReLU())\n",
        "        ]))\n",
        "\n",
        "    def forward(self, img):\n",
        "        output = self.f4(img)\n",
        "        return output\n",
        "\n",
        "\n",
        "class F5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F5, self).__init__()\n",
        "\n",
        "        self.f5 = nn.Sequential(OrderedDict([\n",
        "            ('f5', nn.Linear(84, 10)),\n",
        "            ('sig5', nn.LogSoftmax(dim=-1))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, img):\n",
        "        output = self.f5(img)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    Input - 1x32x32\n",
        "    Output - 10\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.c1 = C1()\n",
        "        self.c2_1 = C2()\n",
        "        self.c2_2 = C2()\n",
        "        self.c3 = C3()\n",
        "        self.f4 = F4()\n",
        "        self.f5 = F5()\n",
        "\n",
        "    def forward(self, img):\n",
        "        output = self.c1(img)\n",
        "\n",
        "        x = self.c2_1(output)\n",
        "        output = self.c2_2(output)\n",
        "\n",
        "        output += x\n",
        "\n",
        "        output = self.c3(output)\n",
        "        output = output.view(img.size(0), -1)\n",
        "        output = self.f4(output)\n",
        "        output = self.f5(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST"
      ],
      "metadata": {
        "id": "liUHikXOs3Hg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = MNIST('./data/mnist',\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.Resize((32, 32)),\n",
        "                       transforms.ToTensor()]))\n",
        "data_test = MNIST('./data/mnist',\n",
        "                  train=False,\n",
        "                  download=True,\n",
        "                  transform=transforms.Compose([\n",
        "                      transforms.Resize((32, 32)),\n",
        "                      transforms.ToTensor()]))\n",
        "\n",
        "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)\n",
        "data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcfAVELnoy0g",
        "outputId": "84f9c6d0-87a3-48db-9732-57dcf95b692c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 52114386.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20685740.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 73786823.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14270059.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_loader.dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yavemX9etF0n",
        "outputId": "0eddfa9c-0a31-42a4-bfe0-4a86107dd89d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data/mnist\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjJzs--IapgQ",
        "outputId": "f13ac450-4b05-4384-a420-9ba3ddd67808"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = LeNet5().to(device)"
      ],
      "metadata": {
        "id": "u2qe4C3PtJl0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim"
      ],
      "metadata": {
        "id": "4t_00ryntxYJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "_oR1qEbKtqWe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cur_batch_win = None\n",
        "cur_batch_win_opts = {\n",
        "    'xlabel': 'Batch Number',\n",
        "    'ylabel': 'Loss'\n",
        "}\n"
      ],
      "metadata": {
        "id": "ecciqhYDtwam"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    global cur_batch_win\n",
        "    net.train()\n",
        "    loss_list, batch_list = [], []\n",
        "    for i, (images, labels) in enumerate(data_train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = net(images)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        loss_list.append(loss.detach().cpu().item())\n",
        "        batch_list.append(i+1)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "8LGpxWaJt43Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    net.eval()\n",
        "    total_correct = 0\n",
        "    avg_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(data_test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output = net(images)\n",
        "        avg_loss += criterion(output, labels).sum()\n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "    avg_loss /= len(data_test)\n",
        "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))"
      ],
      "metadata": {
        "id": "7nJd22L0uUSn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as nn"
      ],
      "metadata": {
        "id": "1UHVZ0i-uhwU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test(epoch):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "6YLdbtUFuZm8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visdom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qID-Pzh6vBbv",
        "outputId": "933e6e24-3ad7-41a0-bef2-16b0d9cd982a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.16.0)\n",
            "Collecting jsonpatch (from visdom)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.6.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (9.4.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2023.7.22)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408194 sha256=7098d23a1ba196a72a3cb7419512c0bb40b337b495bd5c58b4d179245327bce7\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built visdom\n",
            "Installing collected packages: jsonpointer, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 visdom-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMH5n4nIvFSV",
        "outputId": "acac6808-ed85-4c93-c098-1f602292b292"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import visdom\n",
        "import onnx"
      ],
      "metadata": {
        "id": "BxQAit-Tu-tV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import time\n",
        "start_time = time.time()\n",
        "print(start_time)\n",
        "for e in range(1, 16):\n",
        "        train_and_test(e)\n",
        "end_time = time.time()\n",
        "print(end_time)\n",
        "%%time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOPELYyvuomY",
        "outputId": "a38d761b-9c6d-48bb-ef9e-40b3325d2ccc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1697241682.1536512\n",
            "Train - Epoch 1, Batch: 0, Loss: 2.308594\n",
            "Train - Epoch 1, Batch: 10, Loss: 0.826480\n",
            "Train - Epoch 1, Batch: 20, Loss: 0.288680\n",
            "Train - Epoch 1, Batch: 30, Loss: 0.294543\n",
            "Train - Epoch 1, Batch: 40, Loss: 0.183906\n",
            "Train - Epoch 1, Batch: 50, Loss: 0.162269\n",
            "Train - Epoch 1, Batch: 60, Loss: 0.157010\n",
            "Train - Epoch 1, Batch: 70, Loss: 0.164065\n",
            "Train - Epoch 1, Batch: 80, Loss: 0.117230\n",
            "Train - Epoch 1, Batch: 90, Loss: 0.229494\n",
            "Train - Epoch 1, Batch: 100, Loss: 0.110717\n",
            "Train - Epoch 1, Batch: 110, Loss: 0.116803\n",
            "Train - Epoch 1, Batch: 120, Loss: 0.134149\n",
            "Train - Epoch 1, Batch: 130, Loss: 0.114909\n",
            "Train - Epoch 1, Batch: 140, Loss: 0.067727\n",
            "Train - Epoch 1, Batch: 150, Loss: 0.058577\n",
            "Train - Epoch 1, Batch: 160, Loss: 0.048983\n",
            "Train - Epoch 1, Batch: 170, Loss: 0.109227\n",
            "Train - Epoch 1, Batch: 180, Loss: 0.095610\n",
            "Train - Epoch 1, Batch: 190, Loss: 0.052695\n",
            "Train - Epoch 1, Batch: 200, Loss: 0.060786\n",
            "Train - Epoch 1, Batch: 210, Loss: 0.040665\n",
            "Train - Epoch 1, Batch: 220, Loss: 0.068115\n",
            "Train - Epoch 1, Batch: 230, Loss: 0.068247\n",
            "Test Avg. Loss: 0.000066, Accuracy: 0.981300\n",
            "Train - Epoch 2, Batch: 0, Loss: 0.061020\n",
            "Train - Epoch 2, Batch: 10, Loss: 0.033490\n",
            "Train - Epoch 2, Batch: 20, Loss: 0.038648\n",
            "Train - Epoch 2, Batch: 30, Loss: 0.071324\n",
            "Train - Epoch 2, Batch: 40, Loss: 0.061714\n",
            "Train - Epoch 2, Batch: 50, Loss: 0.051399\n",
            "Train - Epoch 2, Batch: 60, Loss: 0.040819\n",
            "Train - Epoch 2, Batch: 70, Loss: 0.092433\n",
            "Train - Epoch 2, Batch: 80, Loss: 0.077612\n",
            "Train - Epoch 2, Batch: 90, Loss: 0.049428\n",
            "Train - Epoch 2, Batch: 100, Loss: 0.090538\n",
            "Train - Epoch 2, Batch: 110, Loss: 0.079180\n",
            "Train - Epoch 2, Batch: 120, Loss: 0.107062\n",
            "Train - Epoch 2, Batch: 130, Loss: 0.035053\n",
            "Train - Epoch 2, Batch: 140, Loss: 0.048361\n",
            "Train - Epoch 2, Batch: 150, Loss: 0.051393\n",
            "Train - Epoch 2, Batch: 160, Loss: 0.059639\n",
            "Train - Epoch 2, Batch: 170, Loss: 0.039204\n",
            "Train - Epoch 2, Batch: 180, Loss: 0.066073\n",
            "Train - Epoch 2, Batch: 190, Loss: 0.038472\n",
            "Train - Epoch 2, Batch: 200, Loss: 0.092005\n",
            "Train - Epoch 2, Batch: 210, Loss: 0.029959\n",
            "Train - Epoch 2, Batch: 220, Loss: 0.056880\n",
            "Train - Epoch 2, Batch: 230, Loss: 0.056619\n",
            "Test Avg. Loss: 0.000050, Accuracy: 0.984700\n",
            "Train - Epoch 3, Batch: 0, Loss: 0.016943\n",
            "Train - Epoch 3, Batch: 10, Loss: 0.044918\n",
            "Train - Epoch 3, Batch: 20, Loss: 0.037379\n",
            "Train - Epoch 3, Batch: 30, Loss: 0.025709\n",
            "Train - Epoch 3, Batch: 40, Loss: 0.100045\n",
            "Train - Epoch 3, Batch: 50, Loss: 0.044846\n",
            "Train - Epoch 3, Batch: 60, Loss: 0.010714\n",
            "Train - Epoch 3, Batch: 70, Loss: 0.061266\n",
            "Train - Epoch 3, Batch: 80, Loss: 0.039371\n",
            "Train - Epoch 3, Batch: 90, Loss: 0.036459\n",
            "Train - Epoch 3, Batch: 100, Loss: 0.056324\n",
            "Train - Epoch 3, Batch: 110, Loss: 0.037591\n",
            "Train - Epoch 3, Batch: 120, Loss: 0.048830\n",
            "Train - Epoch 3, Batch: 130, Loss: 0.028174\n",
            "Train - Epoch 3, Batch: 140, Loss: 0.062146\n",
            "Train - Epoch 3, Batch: 150, Loss: 0.097780\n",
            "Train - Epoch 3, Batch: 160, Loss: 0.045437\n",
            "Train - Epoch 3, Batch: 170, Loss: 0.065308\n",
            "Train - Epoch 3, Batch: 180, Loss: 0.017662\n",
            "Train - Epoch 3, Batch: 190, Loss: 0.018161\n",
            "Train - Epoch 3, Batch: 200, Loss: 0.038645\n",
            "Train - Epoch 3, Batch: 210, Loss: 0.012063\n",
            "Train - Epoch 3, Batch: 220, Loss: 0.021729\n",
            "Train - Epoch 3, Batch: 230, Loss: 0.010957\n",
            "Test Avg. Loss: 0.000045, Accuracy: 0.985400\n",
            "Train - Epoch 4, Batch: 0, Loss: 0.016893\n",
            "Train - Epoch 4, Batch: 10, Loss: 0.015974\n",
            "Train - Epoch 4, Batch: 20, Loss: 0.052551\n",
            "Train - Epoch 4, Batch: 30, Loss: 0.015345\n",
            "Train - Epoch 4, Batch: 40, Loss: 0.071967\n",
            "Train - Epoch 4, Batch: 50, Loss: 0.039031\n",
            "Train - Epoch 4, Batch: 60, Loss: 0.035453\n",
            "Train - Epoch 4, Batch: 70, Loss: 0.088448\n",
            "Train - Epoch 4, Batch: 80, Loss: 0.022671\n",
            "Train - Epoch 4, Batch: 90, Loss: 0.020085\n",
            "Train - Epoch 4, Batch: 100, Loss: 0.047808\n",
            "Train - Epoch 4, Batch: 110, Loss: 0.054845\n",
            "Train - Epoch 4, Batch: 120, Loss: 0.066117\n",
            "Train - Epoch 4, Batch: 130, Loss: 0.059994\n",
            "Train - Epoch 4, Batch: 140, Loss: 0.008258\n",
            "Train - Epoch 4, Batch: 150, Loss: 0.071153\n",
            "Train - Epoch 4, Batch: 160, Loss: 0.019863\n",
            "Train - Epoch 4, Batch: 170, Loss: 0.021502\n",
            "Train - Epoch 4, Batch: 180, Loss: 0.053574\n",
            "Train - Epoch 4, Batch: 190, Loss: 0.014805\n",
            "Train - Epoch 4, Batch: 200, Loss: 0.098514\n",
            "Train - Epoch 4, Batch: 210, Loss: 0.038598\n",
            "Train - Epoch 4, Batch: 220, Loss: 0.051567\n",
            "Train - Epoch 4, Batch: 230, Loss: 0.107205\n",
            "Test Avg. Loss: 0.000038, Accuracy: 0.988900\n",
            "Train - Epoch 5, Batch: 0, Loss: 0.024144\n",
            "Train - Epoch 5, Batch: 10, Loss: 0.035607\n",
            "Train - Epoch 5, Batch: 20, Loss: 0.004677\n",
            "Train - Epoch 5, Batch: 30, Loss: 0.021510\n",
            "Train - Epoch 5, Batch: 40, Loss: 0.024727\n",
            "Train - Epoch 5, Batch: 50, Loss: 0.067786\n",
            "Train - Epoch 5, Batch: 60, Loss: 0.073103\n",
            "Train - Epoch 5, Batch: 70, Loss: 0.066757\n",
            "Train - Epoch 5, Batch: 80, Loss: 0.033783\n",
            "Train - Epoch 5, Batch: 90, Loss: 0.019665\n",
            "Train - Epoch 5, Batch: 100, Loss: 0.021059\n",
            "Train - Epoch 5, Batch: 110, Loss: 0.014206\n",
            "Train - Epoch 5, Batch: 120, Loss: 0.016834\n",
            "Train - Epoch 5, Batch: 130, Loss: 0.065857\n",
            "Train - Epoch 5, Batch: 140, Loss: 0.030756\n",
            "Train - Epoch 5, Batch: 150, Loss: 0.034311\n",
            "Train - Epoch 5, Batch: 160, Loss: 0.053560\n",
            "Train - Epoch 5, Batch: 170, Loss: 0.012059\n",
            "Train - Epoch 5, Batch: 180, Loss: 0.042946\n",
            "Train - Epoch 5, Batch: 190, Loss: 0.012306\n",
            "Train - Epoch 5, Batch: 200, Loss: 0.020473\n",
            "Train - Epoch 5, Batch: 210, Loss: 0.046101\n",
            "Train - Epoch 5, Batch: 220, Loss: 0.046310\n",
            "Train - Epoch 5, Batch: 230, Loss: 0.058904\n",
            "Test Avg. Loss: 0.000042, Accuracy: 0.987000\n",
            "Train - Epoch 6, Batch: 0, Loss: 0.008741\n",
            "Train - Epoch 6, Batch: 10, Loss: 0.027695\n",
            "Train - Epoch 6, Batch: 20, Loss: 0.044174\n",
            "Train - Epoch 6, Batch: 30, Loss: 0.004632\n",
            "Train - Epoch 6, Batch: 40, Loss: 0.064430\n",
            "Train - Epoch 6, Batch: 50, Loss: 0.027395\n",
            "Train - Epoch 6, Batch: 60, Loss: 0.054933\n",
            "Train - Epoch 6, Batch: 70, Loss: 0.020829\n",
            "Train - Epoch 6, Batch: 80, Loss: 0.014846\n",
            "Train - Epoch 6, Batch: 90, Loss: 0.006907\n",
            "Train - Epoch 6, Batch: 100, Loss: 0.024053\n",
            "Train - Epoch 6, Batch: 110, Loss: 0.033749\n",
            "Train - Epoch 6, Batch: 120, Loss: 0.032151\n",
            "Train - Epoch 6, Batch: 130, Loss: 0.008052\n",
            "Train - Epoch 6, Batch: 140, Loss: 0.029043\n",
            "Train - Epoch 6, Batch: 150, Loss: 0.030353\n",
            "Train - Epoch 6, Batch: 160, Loss: 0.023392\n",
            "Train - Epoch 6, Batch: 170, Loss: 0.017404\n",
            "Train - Epoch 6, Batch: 180, Loss: 0.014505\n",
            "Train - Epoch 6, Batch: 190, Loss: 0.069132\n",
            "Train - Epoch 6, Batch: 200, Loss: 0.030481\n",
            "Train - Epoch 6, Batch: 210, Loss: 0.025652\n",
            "Train - Epoch 6, Batch: 220, Loss: 0.064050\n",
            "Train - Epoch 6, Batch: 230, Loss: 0.017366\n",
            "Test Avg. Loss: 0.000045, Accuracy: 0.987400\n",
            "Train - Epoch 7, Batch: 0, Loss: 0.026328\n",
            "Train - Epoch 7, Batch: 10, Loss: 0.023747\n",
            "Train - Epoch 7, Batch: 20, Loss: 0.027785\n",
            "Train - Epoch 7, Batch: 30, Loss: 0.014153\n",
            "Train - Epoch 7, Batch: 40, Loss: 0.048384\n",
            "Train - Epoch 7, Batch: 50, Loss: 0.135534\n",
            "Train - Epoch 7, Batch: 60, Loss: 0.002625\n",
            "Train - Epoch 7, Batch: 70, Loss: 0.013122\n",
            "Train - Epoch 7, Batch: 80, Loss: 0.034371\n",
            "Train - Epoch 7, Batch: 90, Loss: 0.016719\n",
            "Train - Epoch 7, Batch: 100, Loss: 0.039205\n",
            "Train - Epoch 7, Batch: 110, Loss: 0.049097\n",
            "Train - Epoch 7, Batch: 120, Loss: 0.006731\n",
            "Train - Epoch 7, Batch: 130, Loss: 0.032685\n",
            "Train - Epoch 7, Batch: 140, Loss: 0.040074\n",
            "Train - Epoch 7, Batch: 150, Loss: 0.003627\n",
            "Train - Epoch 7, Batch: 160, Loss: 0.016222\n",
            "Train - Epoch 7, Batch: 170, Loss: 0.051016\n",
            "Train - Epoch 7, Batch: 180, Loss: 0.020708\n",
            "Train - Epoch 7, Batch: 190, Loss: 0.007028\n",
            "Train - Epoch 7, Batch: 200, Loss: 0.017801\n",
            "Train - Epoch 7, Batch: 210, Loss: 0.020858\n",
            "Train - Epoch 7, Batch: 220, Loss: 0.057051\n",
            "Train - Epoch 7, Batch: 230, Loss: 0.015604\n",
            "Test Avg. Loss: 0.000045, Accuracy: 0.987700\n",
            "Train - Epoch 8, Batch: 0, Loss: 0.048176\n",
            "Train - Epoch 8, Batch: 10, Loss: 0.058152\n",
            "Train - Epoch 8, Batch: 20, Loss: 0.020475\n",
            "Train - Epoch 8, Batch: 30, Loss: 0.054798\n",
            "Train - Epoch 8, Batch: 40, Loss: 0.007267\n",
            "Train - Epoch 8, Batch: 50, Loss: 0.033094\n",
            "Train - Epoch 8, Batch: 60, Loss: 0.009441\n",
            "Train - Epoch 8, Batch: 70, Loss: 0.010163\n",
            "Train - Epoch 8, Batch: 80, Loss: 0.015762\n",
            "Train - Epoch 8, Batch: 90, Loss: 0.026490\n",
            "Train - Epoch 8, Batch: 100, Loss: 0.033431\n",
            "Train - Epoch 8, Batch: 110, Loss: 0.004940\n",
            "Train - Epoch 8, Batch: 120, Loss: 0.026158\n",
            "Train - Epoch 8, Batch: 130, Loss: 0.060434\n",
            "Train - Epoch 8, Batch: 140, Loss: 0.011574\n",
            "Train - Epoch 8, Batch: 150, Loss: 0.065673\n",
            "Train - Epoch 8, Batch: 160, Loss: 0.015821\n",
            "Train - Epoch 8, Batch: 170, Loss: 0.019439\n",
            "Train - Epoch 8, Batch: 180, Loss: 0.035712\n",
            "Train - Epoch 8, Batch: 190, Loss: 0.038272\n",
            "Train - Epoch 8, Batch: 200, Loss: 0.006425\n",
            "Train - Epoch 8, Batch: 210, Loss: 0.005689\n",
            "Train - Epoch 8, Batch: 220, Loss: 0.052132\n",
            "Train - Epoch 8, Batch: 230, Loss: 0.060216\n",
            "Test Avg. Loss: 0.000052, Accuracy: 0.986100\n",
            "Train - Epoch 9, Batch: 0, Loss: 0.039735\n",
            "Train - Epoch 9, Batch: 10, Loss: 0.057050\n",
            "Train - Epoch 9, Batch: 20, Loss: 0.022657\n",
            "Train - Epoch 9, Batch: 30, Loss: 0.033306\n",
            "Train - Epoch 9, Batch: 40, Loss: 0.008582\n",
            "Train - Epoch 9, Batch: 50, Loss: 0.013429\n",
            "Train - Epoch 9, Batch: 60, Loss: 0.044935\n",
            "Train - Epoch 9, Batch: 70, Loss: 0.013787\n",
            "Train - Epoch 9, Batch: 80, Loss: 0.004573\n",
            "Train - Epoch 9, Batch: 90, Loss: 0.012530\n",
            "Train - Epoch 9, Batch: 100, Loss: 0.054247\n",
            "Train - Epoch 9, Batch: 110, Loss: 0.016561\n",
            "Train - Epoch 9, Batch: 120, Loss: 0.023137\n",
            "Train - Epoch 9, Batch: 130, Loss: 0.007821\n",
            "Train - Epoch 9, Batch: 140, Loss: 0.032570\n",
            "Train - Epoch 9, Batch: 150, Loss: 0.030664\n",
            "Train - Epoch 9, Batch: 160, Loss: 0.015911\n",
            "Train - Epoch 9, Batch: 170, Loss: 0.021714\n",
            "Train - Epoch 9, Batch: 180, Loss: 0.036967\n",
            "Train - Epoch 9, Batch: 190, Loss: 0.001200\n",
            "Train - Epoch 9, Batch: 200, Loss: 0.023299\n",
            "Train - Epoch 9, Batch: 210, Loss: 0.032149\n",
            "Train - Epoch 9, Batch: 220, Loss: 0.016790\n",
            "Train - Epoch 9, Batch: 230, Loss: 0.023846\n",
            "Test Avg. Loss: 0.000053, Accuracy: 0.987600\n",
            "Train - Epoch 10, Batch: 0, Loss: 0.013906\n",
            "Train - Epoch 10, Batch: 10, Loss: 0.010571\n",
            "Train - Epoch 10, Batch: 20, Loss: 0.015424\n",
            "Train - Epoch 10, Batch: 30, Loss: 0.007819\n",
            "Train - Epoch 10, Batch: 40, Loss: 0.018726\n",
            "Train - Epoch 10, Batch: 50, Loss: 0.017660\n",
            "Train - Epoch 10, Batch: 60, Loss: 0.010985\n",
            "Train - Epoch 10, Batch: 70, Loss: 0.030981\n",
            "Train - Epoch 10, Batch: 80, Loss: 0.051488\n",
            "Train - Epoch 10, Batch: 90, Loss: 0.044015\n",
            "Train - Epoch 10, Batch: 100, Loss: 0.005341\n",
            "Train - Epoch 10, Batch: 110, Loss: 0.012095\n",
            "Train - Epoch 10, Batch: 120, Loss: 0.068358\n",
            "Train - Epoch 10, Batch: 130, Loss: 0.012860\n",
            "Train - Epoch 10, Batch: 140, Loss: 0.023730\n",
            "Train - Epoch 10, Batch: 150, Loss: 0.014061\n",
            "Train - Epoch 10, Batch: 160, Loss: 0.026593\n",
            "Train - Epoch 10, Batch: 170, Loss: 0.012426\n",
            "Train - Epoch 10, Batch: 180, Loss: 0.047804\n",
            "Train - Epoch 10, Batch: 190, Loss: 0.048599\n",
            "Train - Epoch 10, Batch: 200, Loss: 0.043865\n",
            "Train - Epoch 10, Batch: 210, Loss: 0.023976\n",
            "Train - Epoch 10, Batch: 220, Loss: 0.010898\n",
            "Train - Epoch 10, Batch: 230, Loss: 0.026324\n",
            "Test Avg. Loss: 0.000051, Accuracy: 0.986400\n",
            "Train - Epoch 11, Batch: 0, Loss: 0.013389\n",
            "Train - Epoch 11, Batch: 10, Loss: 0.044337\n",
            "Train - Epoch 11, Batch: 20, Loss: 0.017779\n",
            "Train - Epoch 11, Batch: 30, Loss: 0.019168\n",
            "Train - Epoch 11, Batch: 40, Loss: 0.057174\n",
            "Train - Epoch 11, Batch: 50, Loss: 0.012373\n",
            "Train - Epoch 11, Batch: 60, Loss: 0.031518\n",
            "Train - Epoch 11, Batch: 70, Loss: 0.026515\n",
            "Train - Epoch 11, Batch: 80, Loss: 0.020601\n",
            "Train - Epoch 11, Batch: 90, Loss: 0.034287\n",
            "Train - Epoch 11, Batch: 100, Loss: 0.025972\n",
            "Train - Epoch 11, Batch: 110, Loss: 0.026037\n",
            "Train - Epoch 11, Batch: 120, Loss: 0.001573\n",
            "Train - Epoch 11, Batch: 130, Loss: 0.003520\n",
            "Train - Epoch 11, Batch: 140, Loss: 0.013887\n",
            "Train - Epoch 11, Batch: 150, Loss: 0.027667\n",
            "Train - Epoch 11, Batch: 160, Loss: 0.030746\n",
            "Train - Epoch 11, Batch: 170, Loss: 0.017056\n",
            "Train - Epoch 11, Batch: 180, Loss: 0.004952\n",
            "Train - Epoch 11, Batch: 190, Loss: 0.035299\n",
            "Train - Epoch 11, Batch: 200, Loss: 0.046090\n",
            "Train - Epoch 11, Batch: 210, Loss: 0.040166\n",
            "Train - Epoch 11, Batch: 220, Loss: 0.008686\n",
            "Train - Epoch 11, Batch: 230, Loss: 0.027704\n",
            "Test Avg. Loss: 0.000044, Accuracy: 0.989700\n",
            "Train - Epoch 12, Batch: 0, Loss: 0.004768\n",
            "Train - Epoch 12, Batch: 10, Loss: 0.004205\n",
            "Train - Epoch 12, Batch: 20, Loss: 0.031528\n",
            "Train - Epoch 12, Batch: 30, Loss: 0.033331\n",
            "Train - Epoch 12, Batch: 40, Loss: 0.023715\n",
            "Train - Epoch 12, Batch: 50, Loss: 0.002865\n",
            "Train - Epoch 12, Batch: 60, Loss: 0.044142\n",
            "Train - Epoch 12, Batch: 70, Loss: 0.059358\n",
            "Train - Epoch 12, Batch: 80, Loss: 0.002851\n",
            "Train - Epoch 12, Batch: 90, Loss: 0.026081\n",
            "Train - Epoch 12, Batch: 100, Loss: 0.010942\n",
            "Train - Epoch 12, Batch: 110, Loss: 0.005670\n",
            "Train - Epoch 12, Batch: 120, Loss: 0.017985\n",
            "Train - Epoch 12, Batch: 130, Loss: 0.005380\n",
            "Train - Epoch 12, Batch: 140, Loss: 0.035513\n",
            "Train - Epoch 12, Batch: 150, Loss: 0.001914\n",
            "Train - Epoch 12, Batch: 160, Loss: 0.002326\n",
            "Train - Epoch 12, Batch: 170, Loss: 0.007782\n",
            "Train - Epoch 12, Batch: 180, Loss: 0.024765\n",
            "Train - Epoch 12, Batch: 190, Loss: 0.051284\n",
            "Train - Epoch 12, Batch: 200, Loss: 0.061312\n",
            "Train - Epoch 12, Batch: 210, Loss: 0.016604\n",
            "Train - Epoch 12, Batch: 220, Loss: 0.028875\n",
            "Train - Epoch 12, Batch: 230, Loss: 0.021742\n",
            "Test Avg. Loss: 0.000045, Accuracy: 0.988200\n",
            "Train - Epoch 13, Batch: 0, Loss: 0.001862\n",
            "Train - Epoch 13, Batch: 10, Loss: 0.016904\n",
            "Train - Epoch 13, Batch: 20, Loss: 0.059124\n",
            "Train - Epoch 13, Batch: 30, Loss: 0.040776\n",
            "Train - Epoch 13, Batch: 40, Loss: 0.034725\n",
            "Train - Epoch 13, Batch: 50, Loss: 0.056402\n",
            "Train - Epoch 13, Batch: 60, Loss: 0.010460\n",
            "Train - Epoch 13, Batch: 70, Loss: 0.082991\n",
            "Train - Epoch 13, Batch: 80, Loss: 0.003962\n",
            "Train - Epoch 13, Batch: 90, Loss: 0.013850\n",
            "Train - Epoch 13, Batch: 100, Loss: 0.044629\n",
            "Train - Epoch 13, Batch: 110, Loss: 0.042250\n",
            "Train - Epoch 13, Batch: 120, Loss: 0.040283\n",
            "Train - Epoch 13, Batch: 130, Loss: 0.041743\n",
            "Train - Epoch 13, Batch: 140, Loss: 0.051668\n",
            "Train - Epoch 13, Batch: 150, Loss: 0.057933\n",
            "Train - Epoch 13, Batch: 160, Loss: 0.016276\n",
            "Train - Epoch 13, Batch: 170, Loss: 0.024597\n",
            "Train - Epoch 13, Batch: 180, Loss: 0.019751\n",
            "Train - Epoch 13, Batch: 190, Loss: 0.017523\n",
            "Train - Epoch 13, Batch: 200, Loss: 0.049158\n",
            "Train - Epoch 13, Batch: 210, Loss: 0.051659\n",
            "Train - Epoch 13, Batch: 220, Loss: 0.013065\n",
            "Train - Epoch 13, Batch: 230, Loss: 0.002486\n",
            "Test Avg. Loss: 0.000045, Accuracy: 0.989800\n",
            "Train - Epoch 14, Batch: 0, Loss: 0.019992\n",
            "Train - Epoch 14, Batch: 10, Loss: 0.024109\n",
            "Train - Epoch 14, Batch: 20, Loss: 0.048911\n",
            "Train - Epoch 14, Batch: 30, Loss: 0.018461\n",
            "Train - Epoch 14, Batch: 40, Loss: 0.000960\n",
            "Train - Epoch 14, Batch: 50, Loss: 0.015870\n",
            "Train - Epoch 14, Batch: 60, Loss: 0.013796\n",
            "Train - Epoch 14, Batch: 70, Loss: 0.021202\n",
            "Train - Epoch 14, Batch: 80, Loss: 0.021611\n",
            "Train - Epoch 14, Batch: 90, Loss: 0.033796\n",
            "Train - Epoch 14, Batch: 100, Loss: 0.049548\n",
            "Train - Epoch 14, Batch: 110, Loss: 0.015079\n",
            "Train - Epoch 14, Batch: 120, Loss: 0.021285\n",
            "Train - Epoch 14, Batch: 130, Loss: 0.048004\n",
            "Train - Epoch 14, Batch: 140, Loss: 0.009340\n",
            "Train - Epoch 14, Batch: 150, Loss: 0.009644\n",
            "Train - Epoch 14, Batch: 160, Loss: 0.032326\n",
            "Train - Epoch 14, Batch: 170, Loss: 0.030424\n",
            "Train - Epoch 14, Batch: 180, Loss: 0.033574\n",
            "Train - Epoch 14, Batch: 190, Loss: 0.065208\n",
            "Train - Epoch 14, Batch: 200, Loss: 0.033850\n",
            "Train - Epoch 14, Batch: 210, Loss: 0.036763\n",
            "Train - Epoch 14, Batch: 220, Loss: 0.020897\n",
            "Train - Epoch 14, Batch: 230, Loss: 0.032757\n",
            "Test Avg. Loss: 0.000076, Accuracy: 0.983700\n",
            "Train - Epoch 15, Batch: 0, Loss: 0.007333\n",
            "Train - Epoch 15, Batch: 10, Loss: 0.004787\n",
            "Train - Epoch 15, Batch: 20, Loss: 0.012426\n",
            "Train - Epoch 15, Batch: 30, Loss: 0.027478\n",
            "Train - Epoch 15, Batch: 40, Loss: 0.015295\n",
            "Train - Epoch 15, Batch: 50, Loss: 0.013670\n",
            "Train - Epoch 15, Batch: 60, Loss: 0.013969\n",
            "Train - Epoch 15, Batch: 70, Loss: 0.038286\n",
            "Train - Epoch 15, Batch: 80, Loss: 0.049969\n",
            "Train - Epoch 15, Batch: 90, Loss: 0.017970\n",
            "Train - Epoch 15, Batch: 100, Loss: 0.015387\n",
            "Train - Epoch 15, Batch: 110, Loss: 0.012587\n",
            "Train - Epoch 15, Batch: 120, Loss: 0.021045\n",
            "Train - Epoch 15, Batch: 130, Loss: 0.007156\n",
            "Train - Epoch 15, Batch: 140, Loss: 0.012879\n",
            "Train - Epoch 15, Batch: 150, Loss: 0.008889\n",
            "Train - Epoch 15, Batch: 160, Loss: 0.013598\n",
            "Train - Epoch 15, Batch: 170, Loss: 0.009415\n",
            "Train - Epoch 15, Batch: 180, Loss: 0.002702\n",
            "Train - Epoch 15, Batch: 190, Loss: 0.070124\n",
            "Train - Epoch 15, Batch: 200, Loss: 0.034302\n",
            "Train - Epoch 15, Batch: 210, Loss: 0.020657\n",
            "Train - Epoch 15, Batch: 220, Loss: 0.013730\n",
            "Train - Epoch 15, Batch: 230, Loss: 0.028113\n",
            "Test Avg. Loss: 0.000062, Accuracy: 0.986200\n",
            "1697241863.4333112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%%time` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs(start_time - end_time)/60.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urs4uyxbTDtV",
        "outputId": "b7bfa871-02cd-4a47-fc98-8a54bfc4ebb4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0213276664415996"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfjSLuI0eXuJ"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}